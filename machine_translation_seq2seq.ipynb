{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation with Seq2Seq Model\n",
    "\n",
    "In this notebook, we will explore the fascinating world of **Machine Translation** using a **Seq2Seq model**. Our goal is to build a model that can translate text from one language to another.\n",
    "\n",
    "Key Highlights:\n",
    "\n",
    "1. **Seq2Seq Model**: We will be using a Sequence-to-Sequence model, a type of model that converts an input sequence into an output sequence. It's widely used in tasks such as machine translation, speech recognition, and more.\n",
    "\n",
    "2. **Beam Search**: To improve the quality of our translations, we will implement Beam Search, a heuristic search algorithm that explores the most promising nodes.\n",
    "\n",
    "3. **BLEU Score**: To evaluate the performance of our model, we will use the Bilingual Evaluation Understudy (BLEU) score. It's a popular metric for machine translation that compares the translated text with the reference text.\n",
    "\n",
    "Stay tuned as we dive into the code and unravel the intricacies of machine translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "The data we will be using for this project is the **Bilingual Sentence Pairs** dataset, which can be found at the following link:\n",
    "\n",
    "[https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs](https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs)\n",
    "\n",
    "This dataset contains pairs of sentences in different languages, making it an excellent resource for our machine translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as pl\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab, GloVe\n",
    "from gensim.models import KeyedVectors\n",
    "from typing import Iterable, List, Callable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The data file contains multiple lines of text. Each line contains a pair of sentences, and an attribution information.\n",
    "    The three parts are separated by tab characters. This function reads the data file and returns a data frame.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): the name of the data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: a data frame containing the data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read each line, split it by tab characters, and store the result in a list\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = [line.strip().split('\\t') for line in f.readlines()]\n",
    "        \n",
    "    # Some lines are empty, so we need to remove them\n",
    "    lines = [line for line in lines if len(line) == 3]\n",
    "    \n",
    "    # Convert the list to a data frame\n",
    "    df = pd.DataFrame(lines, columns=['english', 'french', 'attribution'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file\n",
    "df = read_text('Data/fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english   french                                        attribution\n",
       "0     Go.     Va !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1     Go.  Marche.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2     Go.  Bouge !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3     Hi.  Salut !  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "4     Hi.   Salut.  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have total 185583 pairs of sentences.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have total {len(df)} pairs of sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this step, we will preprocess our data to make it suitable for our Seq2Seq model. We will use the spaCy library, which is a powerful tool for natural language processing. Specifically, we will use two models from Spacy: one for English and one for French. \n",
    "\n",
    "The preprocessing steps include:\n",
    "1. Removing punctuation: Punctuation can introduce unnecessary complexity into our model, so we will remove it.\n",
    "2. Converting to lower case: This ensures that our model does not treat the same word in different cases as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models if necessary\n",
    "if not spacy.util.is_package('en_core_web_md'):\n",
    "    spacy.cli.download('en_core_web_md')\n",
    "if not spacy.util.is_package('fr_core_news_md'):\n",
    "    spacy.cli.download('fr_core_news_md')\n",
    "    \n",
    "# Load the models\n",
    "nlp_en = spacy.load('en_core_web_md')\n",
    "nlp_fr = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185583/185583 [00:02<00:00, 72228.88it/s]\n",
      "100%|██████████| 185583/185583 [00:03<00:00, 49892.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Register the tqdm function with pandas to show a progress bar when applying the function to a data frame\n",
    "tqdm.pandas()\n",
    "\n",
    "# Tokenize the English sentences\n",
    "df['english'] = df['english'].progress_apply(lambda x: ' '.join([token.text.lower() for token in nlp_en.tokenizer(x) if token.is_alpha]))\n",
    "\n",
    "# Tokenize the French sentences\n",
    "df['french'] = df['french'].progress_apply(lambda x: ' '.join([token.text.lower() for token in nlp_fr.tokenizer(x) if token.is_alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34374</th>\n",
       "      <td>i was all by myself</td>\n",
       "      <td>étais absolument seul</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113286</th>\n",
       "      <td>tom dragged himself out of bed</td>\n",
       "      <td>tom se traîna hors de son lit</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133457</th>\n",
       "      <td>i wo let anything happen to you</td>\n",
       "      <td>je ne laisserai rien vous arriver</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61625</th>\n",
       "      <td>i got a date tonight</td>\n",
       "      <td>ai un galant ce soir</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>tom came forward</td>\n",
       "      <td>tom est présenté</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                english                             french  \\\n",
       "34374               i was all by myself              étais absolument seul   \n",
       "113286   tom dragged himself out of bed      tom se traîna hors de son lit   \n",
       "133457  i wo let anything happen to you  je ne laisserai rien vous arriver   \n",
       "61625              i got a date tonight               ai un galant ce soir   \n",
       "19659                  tom came forward                   tom est présenté   \n",
       "\n",
       "                                              attribution  \n",
       "34374   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "113286  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "133457  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "61625   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "19659   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText\n",
    "\n",
    "[TorchText](https://pytorch.org/text/stable/index.html) is a PyTorch package that makes text processing easier and more convenient. It provides essential tools for preprocessing text data, including tokenization, building vocabulary, and batching data for input into a model.\n",
    "\n",
    "In this notebook, we will use TorchText for the following tasks:\n",
    "\n",
    "1. **Tokenization**: We will use the `get_tokenizer` function to create a tokenizer that splits our sentences into tokens (words).\n",
    "\n",
    "2. **Building Vocabulary**: We will use the `build_vocab_from_iterator` function to create a vocabulary from our dataset. This vocabulary will map each token to a unique integer, which our model can work with.\n",
    "\n",
    "3. **Text to Integer Sequence Conversion**: We will create a custom function `text_transform` that uses our vocabulary to convert our sentences into sequences of integers.\n",
    "\n",
    "4. **Batching**: When training our model, we will use the `BucketIterator` to create batches of our data. This will automatically handle padding of sequences to the same length within each batch.\n",
    "\n",
    "By using TorchText, we can greatly simplify the preprocessing of our text data and ensure that it is done in a way that is optimal for our PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yield_tokens` function is a generator function that tokenizes text data from an iterable (like a list or a DataFrame column) and yields the tokens one by one.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The function takes two arguments: `data_iter`, which is an iterable of text data, and `tokenizer`, which is a callable (like a function) that takes a string and returns a list of tokens.\n",
    "\n",
    "2. The function iterates over `data_iter`.\n",
    "\n",
    "3. It applies the `tokenizer` to `text`, which splits the text into tokens.\n",
    "\n",
    "4. It then yields these tokens one by one. Because it's a generator function, it doesn't return all tokens at once but yields them one by one. This is memory-efficient when dealing with large amounts of text data.\n",
    "\n",
    "The reason for using this function is to create a stream of tokens from the text data. These tokens are used to build a vocabulary for text processing. The vocabulary maps each unique token to a unique integer, which can be used as input to a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, tokenizer: Callable[[str], List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Yield the tokens from the data iterator.\n",
    "    \n",
    "    Args:\n",
    "        data_iter (Iterable): the data iterator\n",
    "        tokenizer (Callable[[str], List[str]]): the tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: the tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_vocab_from_iterator` function in TorchText is used to build a vocabulary from an iterator that yields list or iterator of tokens. \n",
    "\n",
    "Here's a step-by-step explanation with a visualization example:\n",
    "\n",
    "1. The function takes an iterator of tokenized text data. This iterator could be a list of sentences, where each sentence is a list of tokens.\n",
    "\n",
    "2. The function iterates over this iterator, and for each list of tokens, it adds each token to the vocabulary.\n",
    "\n",
    "3. The vocabulary is essentially a dictionary where each unique token is a key and the corresponding value is a unique integer. The integer values are assigned in the order the tokens are encountered.\n",
    "\n",
    "4. The function returns this vocabulary.\n",
    "\n",
    "Here's a visualization example:\n",
    "\n",
    "Suppose we have the following tokenized text data:\n",
    "\n",
    "```\n",
    "[\n",
    "    ['I', 'love', 'coding'],\n",
    "    ['coding', 'is', 'fun'],\n",
    "    ['I', 'love', 'AI']\n",
    "]\n",
    "```\n",
    "\n",
    "The `build_vocab_from_iterator` function will build the following vocabulary from this data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'I': 0,\n",
    "    'love': 1,\n",
    "    'coding': 2,\n",
    "    'is': 3,\n",
    "    'fun': 4,\n",
    "    'AI': 5\n",
    "}\n",
    "```\n",
    "\n",
    "Note: The actual integer values may be different depending on the special tokens you add to the vocabulary (like `<unk>`, `<pad>`, `<sos>`, and `<eos>`), but the concept is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build English vocabulary\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(df['english'], en_tokenizer), specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
    "fr_vocab = build_vocab_from_iterator(yield_tokens(df['french'], fr_tokenizer), specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
    "\n",
    "# Default index is the index of <unk>\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])\n",
    "fr_vocab.set_default_index(fr_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 14372, first 10 tokens: ['<pad>', '<sos>', '<eos>', '<unk>', 'i', 'you', 'to', 'the', 'a', 'do']\n",
      "French vocabulary size: 24666, first 10 tokens: ['<pad>', '<sos>', '<eos>', '<unk>', 'je', 'de', 'pas', 'est', 'que', 'à']\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the vocabularies and some first tokens\n",
    "print(f'English vocabulary size: {len(en_vocab)}, first 10 tokens: {list(en_vocab.get_itos())[:10]}')\n",
    "print(f'French vocabulary size: {len(fr_vocab)}, first 10 tokens: {list(fr_vocab.get_itos())[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transform(vocab: Vocab, tokenizer: Callable[[str], List[str]], text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Transform a text into a list of integers.\n",
    "    \n",
    "    Args:\n",
    "        vocab (Vocab): the vocabulary\n",
    "        tokenizer (Callable[[str], List[str]]): the tokenizer\n",
    "        text (str): the input text\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: the list of integers\n",
    "    \"\"\"\n",
    "    \n",
    "    return [vocab[token] for token in tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185583/185583 [00:02<00:00, 63239.09it/s]\n",
      "100%|██████████| 185583/185583 [00:02<00:00, 71976.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the text transforms by adding <sos> and <eos> tokens, and converting the text to a list of integers\n",
    "text_transform_en = lambda text: [en_vocab['<sos>']] + text_transform(en_vocab, en_tokenizer, text) + [en_vocab['<eos>']]\n",
    "text_transform_fr = lambda text: [fr_vocab['<sos>']] + text_transform(fr_vocab, fr_tokenizer, text) + [fr_vocab['<eos>']]\n",
    "\n",
    "# Transform the English and French sentences\n",
    "df['english_transform'] = df['english'].progress_apply(text_transform_en)\n",
    "df['french_transform'] = df['french'].progress_apply(text_transform_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "      <th>english_transform</th>\n",
       "      <th>french_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101201</th>\n",
       "      <td>would you be friends with me</td>\n",
       "      <td>voudriez être ami avec moi</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>[1, 60, 5, 28, 187, 35, 19, 2]</td>\n",
       "      <td>[1, 686, 47, 243, 41, 61, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84048</th>\n",
       "      <td>keep your eyes on the road</td>\n",
       "      <td>garde les yeux sur la route</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "      <td>[1, 178, 26, 428, 34, 7, 752, 2]</td>\n",
       "      <td>[1, 748, 24, 397, 66, 13, 625, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126839</th>\n",
       "      <td>have you ever kissed another girl</td>\n",
       "      <td>as tu déjà embrassé une autre nana</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>[1, 17, 5, 192, 876, 300, 366, 2]</td>\n",
       "      <td>[1, 48, 15, 151, 1511, 23, 126, 3061, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99404</th>\n",
       "      <td>the cliff is almost vertical</td>\n",
       "      <td>la falaise est presque verticale</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "      <td>[1, 7, 3612, 10, 305, 7977, 2]</td>\n",
       "      <td>[1, 13, 5049, 7, 293, 15726, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66397</th>\n",
       "      <td>he is stronger than i am</td>\n",
       "      <td>il a plus de force que moi</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>[1, 14, 10, 1699, 98, 4, 114, 2]</td>\n",
       "      <td>[1, 12, 17, 32, 5, 1497, 8, 61, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  english                              french  \\\n",
       "101201       would you be friends with me          voudriez être ami avec moi   \n",
       "84048          keep your eyes on the road         garde les yeux sur la route   \n",
       "126839  have you ever kissed another girl  as tu déjà embrassé une autre nana   \n",
       "99404        the cliff is almost vertical    la falaise est presque verticale   \n",
       "66397            he is stronger than i am          il a plus de force que moi   \n",
       "\n",
       "                                              attribution  \\\n",
       "101201  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n",
       "84048   CC-BY 2.0 (France) Attribution: tatoeba.org #1...   \n",
       "126839  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "99404   CC-BY 2.0 (France) Attribution: tatoeba.org #4...   \n",
       "66397   CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "\n",
       "                        english_transform  \\\n",
       "101201     [1, 60, 5, 28, 187, 35, 19, 2]   \n",
       "84048    [1, 178, 26, 428, 34, 7, 752, 2]   \n",
       "126839  [1, 17, 5, 192, 876, 300, 366, 2]   \n",
       "99404      [1, 7, 3612, 10, 305, 7977, 2]   \n",
       "66397    [1, 14, 10, 1699, 98, 4, 114, 2]   \n",
       "\n",
       "                                french_transform  \n",
       "101201              [1, 686, 47, 243, 41, 61, 2]  \n",
       "84048          [1, 748, 24, 397, 66, 13, 625, 2]  \n",
       "126839  [1, 48, 15, 151, 1511, 23, 126, 3061, 2]  \n",
       "99404            [1, 13, 5049, 7, 293, 15726, 2]  \n",
       "66397         [1, 12, 17, 32, 5, 1497, 8, 61, 2]  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the maximum length of the English and French sentences\n",
    "en_max_len = df['english_transform'].apply(len).max()\n",
    "fr_max_len = df['french_transform'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of English sentences: 46\n",
      "Maximum length of French sentences: 57\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum length of English sentences: {en_max_len}')\n",
    "print(f'Maximum length of French sentences: {fr_max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pad_sequence` function is used to ensure that all sequences in a batch have the same length by padding shorter sequences with a specific value, usually 0.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The function takes two arguments: `sequence`, which is a list of integers representing a sequence, and `max_length`, which is the desired length for all sequences.\n",
    "\n",
    "2. The function checks if the length of `sequence` is less than `max_length`.\n",
    "\n",
    "3. If it is, the function appends the padding value (`<pad>` token, represented by the integer 0) to `sequence` until its length is equal to `max_length`.\n",
    "\n",
    "4. The function then returns the padded sequence.\n",
    "\n",
    "Here's a visualization example:\n",
    "\n",
    "Suppose we have the following sequence and max_length:\n",
    "\n",
    "```python\n",
    "sequence = [1, 3, 2]\n",
    "max_length = 5\n",
    "```\n",
    "\n",
    "The `pad_sequence` function will pad the sequence with 0s until its length is 5:\n",
    "\n",
    "```python\n",
    "padded_sequence = [0, 0, 1, 3, 2]\n",
    "```\n",
    "\n",
    "This is useful in batch processing where all sequences need to have the same length for the computations to work. The padding value 0 is typically ignored by the model during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence: List[int], max_len: int, vocab: Vocab, pad_first: bool = True) -> List[int]:\n",
    "    \"\"\"\n",
    "    Pad a sequence with <pad> tokens.\n",
    "    \n",
    "    Args:\n",
    "        sequence (List[int]): the input sequence\n",
    "        max_len (int): the maximum length\n",
    "        vocab (Vocab): the vocabulary\n",
    "        pad_first (bool): whether to pad at the beginning or the end\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: the padded sequence as a tensor of long integers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the number of tokens to pad\n",
    "    pad_len = max_len - len(sequence)\n",
    "    \n",
    "    # Pad the sequence\n",
    "    if pad_first:\n",
    "        sequence = [vocab['<pad>']] * pad_len + sequence\n",
    "    else:\n",
    "        sequence = sequence + [vocab['<pad>']] * pad_len\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM models, the order of the sequence matters because the LSTM maintains an internal state that is updated for each element in the sequence. If you pad at the end of the sequence, the LSTM will update its state based on these padding tokens, which are meaningless and could potentially lead to less accurate predictions.\n",
    "\n",
    "On the other hand, if you pad at the beginning of the sequence, the LSTM will start updating its state based on the meaningful tokens right away, as soon as it encounters them. The padding tokens at the beginning of the sequence will have less impact on the final state of the LSTM, leading to more accurate predictions.\n",
    "\n",
    "This is especially important when using LSTM models with a fixed maximum sequence length, where sequences shorter than the maximum length need to be padded. By padding at the beginning of the sequence, you ensure that the LSTM's state is influenced as much as possible by the meaningful tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185583/185583 [00:00<00:00, 790142.52it/s]\n",
      "100%|██████████| 185583/185583 [00:00<00:00, 283204.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pad the English and French sentences\n",
    "# We pad first for the English sentences, and pad last for the French sentences\n",
    "df['english_transform'] = df['english_transform'].progress_apply(lambda x: pad_sequence(x, en_max_len, en_vocab, True))\n",
    "df['french_transform'] = df['french_transform'].progress_apply(lambda x: pad_sequence(x, fr_max_len, fr_vocab, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "      <th>english_transform</th>\n",
       "      <th>french_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107246</th>\n",
       "      <td>we have the best food in town</td>\n",
       "      <td>nous avons la meilleure nourriture de la ville</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 21, 79, 13, 691, 400, 5, 13, 297, 2, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>please take my advice</td>\n",
       "      <td>il vous plaît suivez mon conseil</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 12, 14, 169, 2792, 44, 693, 2, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112857</th>\n",
       "      <td>the tire on my bicycle is flat</td>\n",
       "      <td>le pneu de mon vélo est à plat</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 11, 2779, 5, 44, 454, 7, 9, 2094, 2, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145577</th>\n",
       "      <td>i have done this since high school</td>\n",
       "      <td>je ai plus fait ça depuis le lycée</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 4, 19, 32, 39, 30, 223, 11, 1371, 2, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142422</th>\n",
       "      <td>if i were you i would paint it blue</td>\n",
       "      <td>si étais vous je le peindrais en bleu</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 49, 97, 14, 4, 11, 9717, 22, 1508, 2, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english  \\\n",
       "107246        we have the best food in town   \n",
       "48730                 please take my advice   \n",
       "112857       the tire on my bicycle is flat   \n",
       "145577   i have done this since high school   \n",
       "142422  if i were you i would paint it blue   \n",
       "\n",
       "                                                french  \\\n",
       "107246  nous avons la meilleure nourriture de la ville   \n",
       "48730                 il vous plaît suivez mon conseil   \n",
       "112857                  le pneu de mon vélo est à plat   \n",
       "145577              je ai plus fait ça depuis le lycée   \n",
       "142422           si étais vous je le peindrais en bleu   \n",
       "\n",
       "                                              attribution  \\\n",
       "107246  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n",
       "48730   CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "112857  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   \n",
       "145577  CC-BY 2.0 (France) Attribution: tatoeba.org #3...   \n",
       "142422  CC-BY 2.0 (France) Attribution: tatoeba.org #3...   \n",
       "\n",
       "                                        english_transform  \\\n",
       "107246  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "48730   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "112857  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "145577  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "142422  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         french_transform  \n",
       "107246  [1, 21, 79, 13, 691, 400, 5, 13, 297, 2, 0, 0,...  \n",
       "48730   [1, 12, 14, 169, 2792, 44, 693, 2, 0, 0, 0, 0,...  \n",
       "112857  [1, 11, 2779, 5, 44, 454, 7, 9, 2094, 2, 0, 0,...  \n",
       "145577  [1, 4, 19, 32, 39, 30, 223, 11, 1371, 2, 0, 0,...  \n",
       "142422  [1, 49, 97, 14, 4, 11, 9717, 22, 1508, 2, 0, 0...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english, french):        \n",
    "        # Convert the lists of integers to tensors\n",
    "        self.english = english.apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
    "        self.french = french.apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'english': self.english[idx],\n",
    "            'french': self.french[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df: pd.DataFrame, batch_size: int = 128):\n",
    "        \"\"\"\n",
    "        Initialize the data module.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): the data frame\n",
    "            batch_size (int): the batch size\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Setup the data module. This function will run before training.\n",
    "        \"\"\"\n",
    "        # Create datasets\n",
    "        dataset = TranslationDataset(self.df['english_transform'], self.df['french_transform'])\n",
    "        \n",
    "        # Calculate the size of the training and validation sets\n",
    "        train_size = int(len(dataset) * 0.8)\n",
    "        val_size = len(dataset) - train_size\n",
    "        \n",
    "        # Split the dataset into training and validation sets\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Return the training data loader.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Return the validation data loader.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data module\n",
    "translation_data_module = TranslationDataModule(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "The **Seq2Seq model**, also known as the **Sequence-to-Sequence model**, is a type of model that converts an input sequence into an output sequence. It's widely used in tasks such as machine translation, speech recognition, and more.\n",
    "\n",
    "The Seq2Seq model consists of two main components:\n",
    "\n",
    "1. **Encoder**: The encoder processes the input sequence and returns its own internal state. For each input element, the encoder updates its state. After processing the entire input sequence, the encoder outputs its final state, which serves as the \"context\" of the sequence.\n",
    "\n",
    "2. **Decoder**: The decoder uses the context (the final state of the encoder) to produce the output sequence. The decoder is also a recurrent network, and it produces the output sequence element by element. For each step, the input to the decoder is the previous element, the output of the decoder from the previous step, and the context.\n",
    "\n",
    "Here's a visualization of the Seq2Seq model:\n",
    "\n",
    "```\n",
    "Input Sequence -> | Encoder | -> Context -> | Decoder | -> Output Sequence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer\n",
    "\n",
    "The embedding layer in a neural network is used to transform sparse categorical data, like words in a text dataset, into a dense vector representation that the network can work with. There are two main ways to use the embedding layer:\n",
    "\n",
    "1. **Self-Trained Embeddings**: In this case, the embedding layer is initialized with random weights and learns an embedding for each word in the vocabulary during the training of the network. This is a good option when you don't have a lot of domain-specific knowledge about the relationships between your categories.\n",
    "\n",
    "2. **Pre-Trained Embeddings**: In this case, the embedding layer is initialized with the weights from a pre-trained embedding, like Word2Vec or GloVe. These embeddings are trained on large corpora and can capture a lot of semantic information about words. This is a good option when your dataset is small and you want to leverage external knowledge.\n",
    "\n",
    "Here's example code for these two cases:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Vocabulary size and embedding dimension\n",
    "vocab_size = 5000\n",
    "embed_dim = 300\n",
    "\n",
    "# 1. Self-Trained Embeddings\n",
    "self_trained_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# 2. Pre-Trained Embeddings\n",
    "# Load pre-trained embeddings (replace with actual code to load your embeddings)\n",
    "pretrained_embeddings = torch.randn(vocab_size, embed_dim)  # Note: we need to load from a pretrained model\n",
    "pretrained_embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "```\n",
    "\n",
    "In the self-trained embeddings example, the `nn.Embedding` layer is initialized with random weights. In the pre-trained embeddings example, the `nn.Embedding` layer is initialized with weights from `pretrained_embeddings`, which is a tensor that you would typically load from a pre-trained embedding file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the Pre-Trained Embedding Layer\n",
    "\n",
    "When using pre-trained embeddings, we have two options:\n",
    "\n",
    "1. **Freeze the Embedding Layer**: In this case, the weights of the pre-trained embedding layer are kept constant during training. This means that the semantic information captured by the pre-trained embeddings is preserved, and the model cannot modify these embeddings to better fit the training data. This is a good option when your dataset is small and you want to leverage the semantic information in the pre-trained embeddings as much as possible.\n",
    "\n",
    "2. **Fine-Tune the Embedding Layer**: In this case, the weights of the pre-trained embedding layer are updated during training. This means that the model can modify these embeddings to better fit the training data. This is a good option when your dataset is large and you believe that the pre-trained embeddings may not be optimal for your specific task.\n",
    "\n",
    "You can decide whether to freeze the pre-trained embedding layer by setting the `requires_grad` attribute of the embedding layer's parameters. If `requires_grad` is `False`, the parameters are frozen and will not be updated during training. If `requires_grad` is `True`, the parameters will be updated during training.\n",
    "\n",
    "Here's example code showing how to freeze and unfreeze the pre-trained embedding layer:\n",
    "\n",
    "```python\n",
    "# Freeze the pre-trained embedding layer\n",
    "for param in pretrained_embedding.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the pre-trained embedding layer\n",
    "for param in pretrained_embedding.parameters():\n",
    "    param.requires_grad = True\n",
    "```\n",
    "\n",
    "In the first block of code, the `requires_grad` attribute of the embedding layer's parameters is set to `False`, freezing the parameters. In the second block of code, the `requires_grad` attribute is set to `True`, allowing the parameters to be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For English, we use the existing GloVe embedding from torchtext\n",
    "en_glove = GloVe(name='6B', dim=300)\n",
    "\n",
    "# Create the embedding matrix\n",
    "en_embedding_matrix = en_glove.get_vecs_by_tokens(en_vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('Data/wiki.multi.fr.vec')\n",
    "\n",
    "# Get the number of words in the model's vocabulary and the size of the embeddings\n",
    "embed_size = word2vec_model.vector_size\n",
    "\n",
    "# Get the list of words in the vocabulary\n",
    "fr_vocab_words = fr_vocab.get_itos()\n",
    "\n",
    "# Initialize embedding matrix\n",
    "fr_embedding_matrix = torch.zeros(len(fr_vocab_words), embed_size)\n",
    "\n",
    "# Fill in the embedding matrix\n",
    "for i, word in enumerate(fr_vocab_words):\n",
    "    # Check if the word is in the Word2Vec model's vocabulary\n",
    "    if word in word2vec_model:\n",
    "        fr_embedding_matrix[i] = torch.tensor(word2vec_model[word])\n",
    "    else:\n",
    "        # If the word is not in the Word2Vec model's vocabulary, leave its embedding as zeros\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self, en_embedding_matrix, fr_embedding_matrix, hidden_size, output_size, max_output_len):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Args:\n",
    "            en_embedding_matrix (torch.Tensor): the embedding matrix for English\n",
    "            fr_embedding_matrix (torch.Tensor): the embedding matrix for French\n",
    "            hidden_size (int): the hidden size\n",
    "            output_size (int): the output size\n",
    "            max_output_len (int): the maximum output length\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.sos_token = fr_vocab['<sos>']\n",
    "        self.eos_token = fr_vocab['<eos>']\n",
    "        self.pad_token = fr_vocab['<pad>']\n",
    "        \n",
    "        # Maximum output length\n",
    "        self.max_output_len = max_output_len\n",
    "        \n",
    "        # Embedding layers\n",
    "        # We allow the model to update the embeddings, so we do not freeze them\n",
    "        self.en_embedding = nn.Embedding.from_pretrained(en_embedding_matrix, freeze=False)\n",
    "        self.fr_embedding = nn.Embedding.from_pretrained(fr_embedding_matrix, freeze=False)\n",
    "        \n",
    "        # Encoder block\n",
    "        self.encoder = nn.LSTM(en_embedding_matrix.shape[1], hidden_size, batch_first=True)\n",
    "        self.encoder_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Decoder block\n",
    "        self.decoder = nn.LSTM(fr_embedding_matrix.shape[1], hidden_size, batch_first=True)\n",
    "        self.decoder_leaky_relu = nn.LeakyReLU()\n",
    "        self.decoder_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def encoder_forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: the output tensor\n",
    "            torch.Tensor: the hidden state\n",
    "            torch.Tensor: the cell state\n",
    "        \"\"\"\n",
    "        \n",
    "        # Embed the input\n",
    "        en_embedded = self.en_embedding(x)\n",
    "        \n",
    "        # Dropout\n",
    "        en_embedded = self.encoder_dropout(en_embedded)\n",
    "        \n",
    "        # Pass through the LSTM layer\n",
    "        encoder_output, (hidden_state, cell_state) = self.encoder(en_embedded)\n",
    "        \n",
    "        # Return the output, hidden state, and cell state\n",
    "        return encoder_output, (hidden_state, cell_state)\n",
    "        \n",
    "    def decoder_forward(self, x, hidden_and_cell):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor\n",
    "            hidden_and_cell (tuple): the hidden state and cell state\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: the output tensor\n",
    "            torch.Tensor: the hidden state\n",
    "            torch.Tensor: the cell state\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass through the French embedding layer\n",
    "        fr_embedded = self.fr_embedding(x)\n",
    "            \n",
    "        # Unpack the hidden state and cell state\n",
    "        hidden_state, cell_state = hidden_and_cell\n",
    "        \n",
    "        # Pass through the LSTM layer\n",
    "        decoder_output, (hidden_state, cell_state) = self.decoder(fr_embedded, (hidden_state, cell_state))\n",
    "        \n",
    "        # The output shape is currently (batch_size, 1, hidden_size)\n",
    "        # We want to change it to (batch_size, hidden_size)\n",
    "        decoder_output = decoder_output.squeeze(1)\n",
    "        \n",
    "                # Apply LeakyReLU\n",
    "        fr_embedded = self.decoder_leaky_relu(fr_embedded)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        decoder_output = self.decoder_fc(decoder_output)\n",
    "        \n",
    "        # Return the output, hidden state, and cell state\n",
    "        return decoder_output, (hidden_state, cell_state)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: the output tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the input through the encoder\n",
    "        encoder_output, (hidden_state, cell_state) = self.encoder_forward(x)\n",
    "        \n",
    "        # Get the batch size\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        \n",
    "        # Prepare the input for the decoder\n",
    "        # The shape of the input should be (batch_size, 1) because we are sending in one token at a time\n",
    "        decoder_input = torch.tensor([[self.sos_token]] * batch_size).to(x.device)\n",
    "        \n",
    "        # Create a list to store the outputs\n",
    "        decoder_outputs = []\n",
    "        for _ in range(self.max_output_len):\n",
    "            # Pass the input through the decoder\n",
    "            # The shape of the output should be (batch_size, output_size)\n",
    "            decoder_output, (hidden_state, cell_state) = self.decoder_forward(decoder_input, (hidden_state, cell_state))\n",
    "        \n",
    "            # Add the output to the list\n",
    "            # The purpose of this storage is to calculate the loss later\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            \n",
    "            # Get the predicted token\n",
    "            # The predicted_token has shape (batch_size,)\n",
    "            predicted_token = decoder_output.argmax(1)\n",
    "            \n",
    "            # Detach the predicted token so that we can use it as input for the next iteration, and the model does not update its gradients\n",
    "            decoder_input = predicted_token.detach()\n",
    "            \n",
    "            # Reshape the predicted token to (batch_size, 1) so that we can use it as input for the next iteration\n",
    "            decoder_input = decoder_input.unsqueeze(1)\n",
    "            \n",
    "        # Stack tensors of shape (batch_size, output_size) in the list to get a tensor of shape (batch_size, max_output_len, output_size)\n",
    "        decoder_outputs = torch.stack(decoder_outputs, dim=1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x = batch['english']\n",
    "        y = batch['french']\n",
    "        \n",
    "        # Get the output\n",
    "        # The shape of the output is (batch_size, max_output_len, output_size)\n",
    "        output = self(x)\n",
    "        \n",
    "        # Reshape the output to (batch_size * max_output_len, output_size)\n",
    "        # This is because we want to calculate the loss for each word\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        \n",
    "        # Reshape the target to (batch_size * max_output_len)\n",
    "        # This is because we want to calculate the loss for each word (the target is the next word)\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=self.pad_token)(output, y)\n",
    "        \n",
    "        # Log the loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x = batch['english']\n",
    "        y = batch['french']\n",
    "        \n",
    "        # Get the output\n",
    "        # The shape of the output is (batch_size, max_output_len, output_size)\n",
    "        output = self(x)\n",
    "        \n",
    "        # Reshape the output to (batch_size * max_output_len, output_size)\n",
    "        # This is because we want to calculate the loss for each word\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        \n",
    "        # Reshape the target to (batch_size * max_output_len)\n",
    "        # This is because we want to calculate the loss for each word (the target is the next word)\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=self.pad_token)(output, y)\n",
    "        \n",
    "        # Log the loss\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "seq2seq = Seq2Seq(en_embedding_matrix, fr_embedding_matrix, hidden_size=300, output_size=len(fr_vocab), max_output_len=fr_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stop_callback = pl.pytorch.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=True)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = pl.pytorch.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name               | Type      | Params\n",
      "-------------------------------------------------\n",
      "0 | en_embedding       | Embedding | 4.3 M \n",
      "1 | fr_embedding       | Embedding | 7.4 M \n",
      "2 | encoder            | LSTM      | 722 K \n",
      "3 | encoder_dropout    | Dropout   | 0     \n",
      "4 | decoder            | LSTM      | 722 K \n",
      "5 | decoder_leaky_relu | LeakyReLU | 0     \n",
      "6 | decoder_fc         | Linear    | 7.4 M \n",
      "-------------------------------------------------\n",
      "20.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.6 M    Total params\n",
      "82.323    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2816a2c93bf7490b8cbd43e0bcdbb109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luanpham/miniconda3/envs/packt_nlp_natural_language_processing_in_python_for_beginners/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/luanpham/miniconda3/envs/packt_nlp_natural_language_processing_in_python_for_beginners/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0650badc4074863b182e0f942815369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574bf44b6b7f4fc8810ec8a71003388d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 3.711\n",
      "Epoch 0, global step 1160: 'val_loss' reached 3.71122 (best 3.71122), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=0-step=1160.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86b9ad5f14d4947b9c45070b68d620e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.719 >= min_delta = 0.0. New best score: 2.992\n",
      "Epoch 1, global step 2320: 'val_loss' reached 2.99199 (best 2.99199), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=1-step=2320.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1c4e24cef74108b2087f3fe714945a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.367 >= min_delta = 0.0. New best score: 2.625\n",
      "Epoch 2, global step 3480: 'val_loss' reached 2.62535 (best 2.62535), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=2-step=3480.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a39479058c441e2bae5827e9b5dad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.199 >= min_delta = 0.0. New best score: 2.427\n",
      "Epoch 3, global step 4640: 'val_loss' reached 2.42662 (best 2.42662), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=3-step=4640.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86efdebcd8d14c0fafb21e82c1cb2222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.121 >= min_delta = 0.0. New best score: 2.306\n",
      "Epoch 4, global step 5800: 'val_loss' reached 2.30573 (best 2.30573), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=4-step=5800.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d11a1ec0f1b475f9d18935dac5fa426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.073 >= min_delta = 0.0. New best score: 2.233\n",
      "Epoch 5, global step 6960: 'val_loss' reached 2.23260 (best 2.23260), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=5-step=6960.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6ad29120fc4a00b28571ff45d98959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 2.186\n",
      "Epoch 6, global step 8120: 'val_loss' reached 2.18633 (best 2.18633), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=6-step=8120.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee0749b72847a9a3e35236e9103db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 2.156\n",
      "Epoch 7, global step 9280: 'val_loss' reached 2.15640 (best 2.15640), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=7-step=9280.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9543a1af835446eaa9c8a6c03e1e2d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 2.139\n",
      "Epoch 8, global step 10440: 'val_loss' reached 2.13874 (best 2.13874), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=8-step=10440.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fbd345cca344cab0062f06a46eebd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 2.129\n",
      "Epoch 9, global step 11600: 'val_loss' reached 2.12904 (best 2.12904), saving model to '/Users/luanpham/Insync/minhluan1590@gmail.com/GoogleDrive/working/Teaching/ttth_natural_language_processing_practice/Chapter_05_Machine_Translation/Seq2seq/lightning_logs/version_3/checkpoints/epoch=9-step=11600.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "trainer = pl.Trainer(max_epochs=100, devices=-1, callbacks=[early_stop_callback, checkpoint_callback])\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq2seq, translation_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 39246), started 0:00:06 ago. (Use '!kill 39246' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5b477b45cdf332b4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5b477b45cdf332b4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Translate a sentence from English to French.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): the input sentence\n",
    "        \n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = ' '.join([token.text.lower() for token in nlp_en.tokenizer(sentence) if token.is_alpha])\n",
    "    \n",
    "    # Transform the sentence\n",
    "    transformed = text_transform_en(tokens)\n",
    "    \n",
    "    # Pad the sentence\n",
    "    padded = pad_sequence(transformed, en_max_len, en_vocab, pad_first=True)\n",
    "    \n",
    "    # Convert the sentence to a tensor\n",
    "    tensor = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(seq2seq.device)\n",
    "    \n",
    "    # Get the output\n",
    "    output = seq2seq(tensor)\n",
    "    \n",
    "    # Get the predicted words\n",
    "    predicted = output.argmax(dim=-1).squeeze(0)\n",
    "    \n",
    "    # Convert the predicted words to a list of integers\n",
    "    predicted = predicted.tolist()\n",
    "    \n",
    "    # Remove the <sos> token\n",
    "    predicted = predicted[1:]\n",
    "    \n",
    "    # Remove the <eos> token\n",
    "    predicted = predicted[:predicted.index(fr_vocab['<eos>'])]\n",
    "    \n",
    "    # Convert the integers to words\n",
    "    predicted = [fr_vocab.get_itos()[idx] for idx in predicted]\n",
    "    \n",
    "    # Join the words\n",
    "    predicted = ' '.join(predicted)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je suis étudiant'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate some sentences\n",
    "translate_sentence('I am a student.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'il y a un chat sur la table'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate some sentences\n",
    "translate_sentence('There is a cat on the table.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packt_nlp_natural_language_processing_in_python_for_beginners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
